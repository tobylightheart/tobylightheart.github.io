---
layout: page
title: Generative Pre-Trained Transformers
permalink: /study/generative-pre-trained-transformers
---

Generative Pre-Trained Transformers (GPT) are a combination of the transformer neural network architecture and the generative pre-training unsupervised learning technique. The GPT, GPT-2 and GPT-3 models are a series of developments by OpenAI for natural language processing. 

## Related Concepts
### Internal Links 
+ [Transformer (Neural Network)]({{site.baseurl}}{% link _study/transformer-neural-network.md %}) 
+ [Generative Pre-Training]({{site.baseurl}}{% link _study/generative-pre-training.md %})


### External Links
* [GPT-2: 1.5B Release](https://openai.com/blog/gpt-2-1-5b-release/): OpenAI
* [OpenAI GPT-2 GitHub](https://github.com/openai/gpt-2): OpenAI
* [The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2): Priya Shree.